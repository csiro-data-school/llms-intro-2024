<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta http-equiv="last-modified" content="2024-11-06 07:52:06 +0000">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- meta "search-domain" used for google site search function google_search() -->
    <meta name="search-domain" value="https://github.com/pages/csiro-data-school/llms-intro-2024">
    <link rel="stylesheet" type="text/css" href="../assets/css/bootstrap.css" />
    <link rel="stylesheet" type="text/css" href="../assets/css/bootstrap-theme.css" />
    <link rel="stylesheet" type="text/css" href="../assets/css/lesson.css" />
    <link rel="stylesheet" type="text/css" href="../assets/css/syntax.css" />
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs"></script>


    



    <!-- Favicons for everyone -->
    <link rel="apple-touch-icon-precomposed" sizes="57x57" href="https://csiro-data-school.github.io/llms-intro-2024/assets/favicons/swc/apple-touch-icon-57x57.png" />
    <link rel="apple-touch-icon-precomposed" sizes="114x114" href="https://csiro-data-school.github.io/llms-intro-2024/assets/favicons/swc/apple-touch-icon-114x114.png" />
    <link rel="apple-touch-icon-precomposed" sizes="72x72" href="https://csiro-data-school.github.io/llms-intro-2024/assets/favicons/swc/apple-touch-icon-72x72.png" />
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://csiro-data-school.github.io/llms-intro-2024/assets/favicons/swc/apple-touch-icon-144x144.png" />
    <link rel="apple-touch-icon-precomposed" sizes="60x60" href="https://csiro-data-school.github.io/llms-intro-2024/assets/favicons/swc/apple-touch-icon-60x60.png" />
    <link rel="apple-touch-icon-precomposed" sizes="120x120" href="https://csiro-data-school.github.io/llms-intro-2024/assets/favicons/swc/apple-touch-icon-120x120.png" />
    <link rel="apple-touch-icon-precomposed" sizes="76x76" href="https://csiro-data-school.github.io/llms-intro-2024/assets/favicons/swc/apple-touch-icon-76x76.png" />
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="https://csiro-data-school.github.io/llms-intro-2024/assets/favicons/swc/apple-touch-icon-152x152.png" />
    <link rel="icon" type="image/png" href="https://csiro-data-school.github.io/llms-intro-2024/assets/favicons/swc/favicon-196x196.png" sizes="196x196" />
    <link rel="icon" type="image/png" href="https://csiro-data-school.github.io/llms-intro-2024/assets/favicons/swc/favicon-96x96.png" sizes="96x96" />
    <link rel="icon" type="image/png" href="https://csiro-data-school.github.io/llms-intro-2024/assets/favicons/swc/favicon-32x32.png" sizes="32x32" />
    <link rel="icon" type="image/png" href="https://csiro-data-school.github.io/llms-intro-2024/assets/favicons/swc/favicon-16x16.png" sizes="16x16" />
    <link rel="icon" type="image/png" href="https://csiro-data-school.github.io/llms-intro-2024/assets/favicons/swc/favicon-128.png" sizes="128x128" />
    <meta name="application-name" content="Software Carpentry - LLMs and Generative AI for Science"/>
    <meta name="msapplication-TileColor" content="#FFFFFF" />
    <meta name="msapplication-TileImage" content="https://csiro-data-school.github.io/llms-intro-2024/assets/favicons/swc/mstile-144x144.png" />
    <meta name="msapplication-square70x70logo" content="https://csiro-data-school.github.io/llms-intro-2024/assets/favicons/swc/mstile-70x70.png" />
    <meta name="msapplication-square150x150logo" content="https://csiro-data-school.github.io/llms-intro-2024/assets/favicons/swc/mstile-150x150.png" />
    <meta name="msapplication-wide310x150logo" content="https://csiro-data-school.github.io/llms-intro-2024/assets/favicons/swc/mstile-310x150.png" />
    <meta name="msapplication-square310x310logo" content="https://csiro-data-school.github.io/llms-intro-2024/assets/favicons/swc/mstile-310x310.png" />


    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
	<script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
	<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
	<![endif]-->
    <title>LLMs and Generative AI for Science: Responsible Use of Generative AI</title>
  </head>
  <body>
    <div class="container">
      
<nav class="navbar navbar-default">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>

      
      
      <a href="https://software-carpentry.org" class="pull-left">
        <img class="navbar-logo" src="../assets/img/swc-icon-blue.svg" alt="Software Carpentry logo" />
      </a>
      

      
      <a class="navbar-brand" href="../">Home</a>

    </div>
    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
      <ul class="nav navbar-nav">

	
        <li><a href="../CODE_OF_CONDUCT.html">Code of Conduct</a></li>

        
	
        <li><a href="../setup.html">Setup</a></li>

        
        <li class="dropdown">
          <a href="../" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Episodes <span class="caret"></span></a>
          <ul class="dropdown-menu">
            
            <li><a href="../01-introducing-llms/index.html">What are LLMs and Generative AI</a></li>
            
            <li><a href="../02-responsible-use/index.html">Responsible Use of Generative AI</a></li>
            
            <li><a href="../03-querying-apis-with-llms/index.html">Querying APIs with LLMs</a></li>
            
            <li><a href="../04-integrating-llms/index.html">Integrating LLMs in scientific workflows</a></li>
            
	    <li role="separator" class="divider"></li>
            <li><a href="../aio.html">All in one page (Beta)</a></li>
          </ul>
        </li>
	

	
	
        <li class="dropdown">
          <a href="../" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Extras <span class="caret"></span></a>
          <ul class="dropdown-menu">
            <li><a href="../reference.html">Reference</a></li>
            
            <li><a href="../about/index.html">About</a></li>
            
            <li><a href="../discuss/index.html">Discussion</a></li>
            
            <li><a href="../figures/index.html">Figures</a></li>
            
            <li><a href="../guide/index.html">Instructor Notes</a></li>
            
          </ul>
        </li>
	

	
        <li><a href="../LICENSE.html">License</a></li>
	
	<li><a href="https://github.com/csiro-data-school/llms-intro-2024/edit/gh-pages/_episodes/02-responsible-use.md">Improve this page <span class="glyphicon glyphicon-pencil" aria-hidden="true"></span></a></li>
	
      </ul>
      <form class="navbar-form navbar-right" role="search" id="search" onsubmit="google_search(); return false;">
        <div class="form-group">
          <input type="text" id="google-search" placeholder="Search..." aria-label="Google site search">
        </div>
      </form>
    </div>
  </div>
</nav>


<div class="row">
  <div class="col-xs-1">
    <h3 class="text-left">
      
      <a href="../01-introducing-llms/index.html"><span class="glyphicon glyphicon-menu-left" aria-hidden="true"></span><span class="sr-only">previous episode</span></a>
      
    </h3>
  </div>
  <div class="col-xs-10">
    
    <h3 class="maintitle"><a href="../">LLMs and Generative AI for Science</a></h3>
    
  </div>
  <div class="col-xs-1">
    <h3 class="text-right">
      
      <a href="../03-querying-apis-with-llms/index.html"><span class="glyphicon glyphicon-menu-right" aria-hidden="true"></span><span class="sr-only">next episode</span></a>
      
    </h3>
  </div>
</div>

<article>
<div class="row">
  <div class="col-md-1">
  </div>
  <div class="col-md-10">
    <h1 class="maintitle">Responsible Use of Generative AI</h1>
  </div>
  <div class="col-md-1">
  </div>
</div>


<blockquote class="objectives">
  <h2>Overview</h2>

  <div class="row">
    <div class="col-md-3">
      <strong>Teaching:</strong> 15 min
      <br/>
      <strong>Exercises:</strong> 20 min
    </div>
    <div class="col-md-9">
      <strong>Questions</strong>
      <ul>
	
	<li><p>How do I use Generative AI safely and responsibly?</p>
</li>
	
      </ul>
    </div>
  </div>

  <div class="row">
    <div class="col-md-3">
    </div>
    <div class="col-md-9">
      <strong>Objectives</strong>
      <ul>
	
	<li><p>Recognize the ethical risks and legal restrictions using LLMs and adopted a considreed approach</p>
</li>
	
	<li><p>Identify risks to privacy and intellectual property when using LLMs</p>
</li>
	
	<li><p>Understand errors and their consequences</p>
</li>
	
	<li><p>Develop strategies for risk mitigation</p>
</li>
	
      </ul>
    </div>
  </div>

</blockquote>

<h2 id="responsible-use-of-llms-and-generative-ai">Responsible Use of LLMs and Generative AI</h2>

<p>As society adapts to the emergence of powerful generative AI, our understanding of risks and responsible use is changing. Use may need to conform to local laws or organizational policies. Additionally, broad ethical reasoning and principles can be applied. LLMs, like many technologies, can be used for harm. Reflect on usage and do not use LLMs to cause harm.</p>

<p>There are many emerging policies and frameworks that are useful for reference, such as <a href="https://www.industry.gov.au/publications/australias-artificial-intelligence-ethics-framework/australias-ai-ethics-principles">Australia’s AI Ethics Principles</a> and <a href="https://www.nist.gov/aisi/artificial-intelligence-safety-institute-consortium-aisic">Artificial Intelligence Safety Institute Consortium</a>.</p>

<h3 id="risks-to-privacy-and-intellectual-property">Risks to Privacy and Intellectual Property</h3>

<p>LLMs are available through a number of different outlets, and policies on how input data may be used vary. In many cases, especially where the use of the LLM is cheap, free, or through public websites, data input may not be private. Input data may be used to train models or may be accessible by the individuals operating the LLM service. Companies usually provide terms and conditions that outline how data will be used. In corporate settings, mandated services may be safest. For science use cases, a wide variety of services might be useful. Extra caution should be used when determining whether Personally Identifiable Information (PII) or intellectual property (IP) should be input into LLMs, in many cases this would not be safe and likley violate ethics and policy.</p>

<h4 id="output-use-restrictions">Output use restrictions</h4>

<p>In addition to the risks of supplying PII or IP into LLMs, you should familiarize yourself with the terms under which you can use the outputs of LLMs. Often, LLM providers allow you to use outputs for a wide variety of purposes, and in some cases, you may own the content generated. Some LLMs are released under research-only licenses, and their outputs may not be used for commercial purposes. As of the time of writing, OpenAI LLMs provide liberal terms allowing many uses and ownership of outputs. Further details are available <a href="https://openai.com/policies/usage-policies/">here</a>.</p>

<h3 id="biases-errors-and-consequences">Biases, Errors and Consequences</h3>

<p>LLMs are trained to be highly reliable and in practice, for many use cases, they make few mistakes. Similarly most LLM providers put a signficiant amount of work into “algining” LLMs to reduce biases and risks related to unethical outputs. Nevertheless, LLMs can make mistakes or display biases.</p>

<p>LLMs are more likely to make mistakes on more complex tasks, and these mistakes can be subtle. You should work with LLMs assuming there may be a mistake or bias in what the LLM outputs. Less sophisticated LLMs like ChatGPT version 3.5 are more likely to produce mistakes than better LLMs like ChatGPT 4.</p>

<h3 id="disclosure">Disclosure</h3>

<p>Many organizations and institutions are grappling with how to treat AI-generated content. In some cases, AI-generated content is outright banned. In other cases, AI-generated content is acceptable but should be disclosed. In some cases, what constitutes AI-generated content is clear. For example, asking ChatGPT to generate an essay on “Common household pets” and then copying that content for direct use, representing it as your own work, would be deceptive and even with attribution, in many cases, it would not be allowed.</p>

<h4 id="what-consitutes-ai-generated-content">What consitutes AI generated content?</h4>

<p>However, what constitutes AI-generated content is not always clear cut. For example, AI maybe used as an encyclopedia-like resource to extract facts or overviews on topics (it’s best to cross-check facts from AIs in this case). These facts then might form the basis for original work in an essay. Similarly, AIs can be used at abstract levels to suggest research approaches, describe suitable structures for documents, or review text and make suggestions or simple corrections. Thus, AI authorship is not clear cut, and disclosure or avoidance of AI content is not simple.</p>

<h4 id="adding-value">Adding Value</h4>

<p>One guiding principle when working with generative AI might be to ask “What am I adding here”. If the content are generated could be regenerated with simply obvious prompts and there is no knowledge added then this would indicate little added value.</p>

<h4 id="evolving-societal-rules-and-standards">Evolving Societal Rules and Standards</h4>

<p>As AIs become ubiquitous and predominant tools, societal attitudes to AI-generated content will change and may become clearer.</p>

<h3 id="strategies-for-mitigating-risk">Strategies for Mitigating Risk</h3>

<h3 id="testing">Testing</h3>

<p>Testing outputs of LLMs is important. It reduces risk and ensures that you understand the content output. Testing can take many forms. You might reason through an LLM’s outputs, critically assessing each of its assertions using logic or cross-checking against other material. Testing of LLM-generated code might be similar to how you’d test your own code: you could write unit tests or try out the produced application, testing different usage scenarios and checking for errors.</p>

<h3 id="understanding-consequences-and-setting-a-risk-tolerance">Understanding Consequences and Setting a Risk Tolerance</h3>

<p>In some cases, you may be able to tolerate some error. For example, if you are learning about trigonometry and you are going to read a wide variety of material, it may be okay if an LLM misleads you, as you will discover this error in further learning or exercises. In other cases, errors could be catastrophic. For example, an LLM may incorrectly analyze a series of medical documents and recommend a dangerous or incorrect treatment plan. Understanding consequences and setting an appropriate risk tolerance when using LLMs is essential.</p>

<h3 id="censoring-data">Censoring Data</h3>

<p>One strategy for using LLMs with sensitive data is to generate mock data, metadata, or data structures that remove any sensitive aspects of the data. This must be approached carefully as, depending on the remaining information content, some data censoring techniques can be reversed. In some cases, it may be safe to provide an LLM with column names and statistical properties of the data in the columns. LLMs can also indirectly assist in censoring data by writing scripts to generate representative datasets. For example, you might ask an LLM to “Generate a python script that reads my CSV and replaces the lat and lon of each location in the location field with random lats and lons”. In this example, it would still be necessary to understand how the suggested script worked and whether it was sufficient to censor your data. In many cases, when directly using LLMs, you won’t want to send all the data to the LLM. Rather, you may be asking the LLM something about querying or processing your data. In that case, it may be sufficient to send the structure, e.g., the column names, along with a broad description of the data to the LLM.</p>

<p>Sensitive data in other circumstances might include things like file paths, and you could remove these and substitute them with placeholders.</p>

<h3 id="working-on-adjacent-problems">Working on Adjacent Problems</h3>

<p>Where there is a risk of exposure of intellectual property or sensitive data, it still may be possible to indirectly work with LLMs. For example, you may need to learn and employ a statistical technique on your dataset. It might be possible to learn this technique and work with the LLM to generate code without any specifics of the work you are doing being provided to the LLM. Caution should be taken as over many interactions with an LLM, it may be possible for an adversary to glean information about your IP or data by looking at trends and context.</p>

<blockquote class="challenge">
  <h2 id="understand-terms-and-conditions">Understand Terms and Conditions</h2>

  <p>Find the privacy terms and conditions for OpenAI’s ChatGPT.</p>

</blockquote>

<blockquote class="challenge">
  <h2 id="trick-an-llm">Trick an LLM</h2>

  <p>Try to find a mistake in an LLM by asking a really tricky question at the edge of your domain knowledge.</p>

</blockquote>

<blockquote class="challenge">
  <h2 id="examples-of-sensitive-data">Examples of Sensitive Data</h2>

  <p>Think of some examples of sensitive code in your context.</p>

</blockquote>

<blockquote class="challenge">
  <h2 id="safe-interactions">Safe Interactions</h2>

  <p>Can you think of any safe ways of engaging an LLM around your problem when there may be a sensitive aspect?</p>

</blockquote>

<blockquote class="challenge">
  <h2 id="testing-results">Testing Results</h2>

  <p>How could you test LLM output in your context?</p>

</blockquote>

<h3 id="note">Note</h3>
<p>GPT-4 was used to rewrite drafts of this material and made minor contributions to the content</p>


<blockquote class="keypoints">
  <h2>Key Points</h2>
  <ul>
    
    <li><p>Your inputs may be used, and sensitive data may be exposed depending on the LLM product used.</p>
</li>
    
    <li><p>LLMs make mistakes that can be subtle; always test and understand the consequences.</p>
</li>
    
    <li><p>Directly generating text with LLMs can be spammy and misleading, but some uses may be appropriate.</p>
</li>
    
    <li><p>Effective strategies can help avoid risks.</p>
</li>
    
    <li><p>LLMs can be safely used in many circumstances even where there are adjacent risks.</p>
</li>
    
  </ul>
</blockquote>

</article>

<div class="row">
  <div class="col-xs-1">
    <h3 class="text-left">
      
      <a href="../01-introducing-llms/index.html"><span class="glyphicon glyphicon-menu-left" aria-hidden="true"></span><span class="sr-only">previous episode</span></a>
      
    </h3>
  </div>
  <div class="col-xs-10">
    
  </div>
  <div class="col-xs-1">
    <h3 class="text-right">
      
      <a href="../03-querying-apis-with-llms/index.html"><span class="glyphicon glyphicon-menu-right" aria-hidden="true"></span><span class="sr-only">next episode</span></a>
      
    </h3>
  </div>
</div>


      
      
<footer>
  <div class="row">
    <div class="col-md-6 copyright" align="left">
	
	Copyright &copy; 2018–2024
	<a href="https://carpentries.org/">The Carpentries</a>
        <br>
        Copyright &copy; 2016–2018
	<a href="https://software-carpentry.org">Software Carpentry Foundation</a>
	
    </div>
    <div class="col-md-6 help-links" align="right">
	
	<a href="https://github.com/csiro-data-school/llms-intro-2024/edit/gh-pages/_episodes/02-responsible-use.md">Edit on GitHub</a>
	
	/
	<a href="https://github.com/csiro-data-school/llms-intro-2024/blob/gh-pages/CONTRIBUTING.md">Contributing</a>
	/
	<a href="https://github.com/csiro-data-school/llms-intro-2024/">Source</a>
	/
	<a href="https://github.com/csiro-data-school/llms-intro-2024/blob/gh-pages/CITATION">Cite</a>
	/
	<a href="mailto:team@carpentries.org">Contact</a>
    </div>
  </div>
  <div class="row">
    <div class="col-md-12" align="center">
      Using <a href="https://github.com/carpentries/styles/">The Carpentries style</a>
      version <a href="https://github.com/carpentries/styles/releases/tag/v9.5.2">9.5.2</a>.
    </div>
  </div>
</footer>

      
    </div>
    
<script src="../assets/js/jquery.min.js"></script>
<script src="../assets/js/bootstrap.min.js"></script>
<script src="../assets/js/lesson.js"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-37305346-2', 'auto');
  ga('send', 'pageview');
</script>

  </body>
</html>
