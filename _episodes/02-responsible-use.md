---
title: "Responsible Use of LLMs and Generative AI"
teaching: 15
exercises: 20
questions:
- "How do I use llms safely and responsibly?"
objectives:
- "Know when using LLMs can be a risk to privacy or IP"
- "Avoiding Errors and Consequences"
- "Reasonable use of LLMs" 
- "Strategies for Mitigating Risks" 
keypoints:
- "Depending on the LLMs product used your inputs maybe used and sensitive data maybe exposed"
- "LLMs make mistakes and they can be subtle, test, understand the consequences of a mistake"
- "Using LLMs to directly generate text can be spammy and misleading but some such uses maybe appropriate"
- "Strategies can help avoid risks"
- "LLMs can be safely used in many circumstances even where there are adjacent risks"
---

## Responsible Use of LLMs and Generative AI

As society adapts to the emergence of powerful generative AI our understanding of risks and responsible use is changing. Your use maybe need to conform to local laws or organisational policies and you should understand and work within these. Additionally broad ethical reasoning and principles can be applied. LLMs like many technologies can be used for harm. Reflect on your usage and do not use LLMs to cause harm.

### Risks to privacy and Intellecutal Property

LLMs are avaialble through a number of different outlets and policies on how data input maybe used vary. In many cases, especially where the use of the LLM is cheap, free, or through public websites, data you input may not be private. Your input data maybe used to train models or maybe accessible by the indviduals operating the LLM service. Normally companies provide terms and conditions that outline how your data will be used. In corporate settings mandated services maybe safest. For science use cases a wide variety of services might be useful. Typically your should not input Personally Identifiable Information (PII) or intellecutal property (IP) into LLMs.

In addition to the risks of supplying PII or IP into LLMs you should familiarize yourself with the terms under which you can use the outputs of LLMs. Often LLM providers allow you to use outputs for a wide variety of purposes and in some cases you may own content generated. In a few cases some LLMs are released under research only licenses and their outputs may not be used for commerical purposes. As of the time of writing openai LLMs provide liberal terms allowing many uses and ownership of outputs, further details are here https://openai.com/policies/usage-policies/. 

### Errors and consequences  

LLMs are trained to be highly reliable and in practice for many use cases they make few mistakes. However there LLMs are more likely to make mistakes on more complex tasks and these mistakes can be subtle. You should work with LLMs assuming there maybe a mistake in what the LLM outputs. Less sophisticated LLMs like ChatGPT version 3.5 are more likely to produce mistakes than better LLMs like ChatGPT 4. LLM developers generally work very hard to reduce mistakes and LLMs are often trained and tested on their ability to produce correct output.

### Strategies for mitigating risk

### Testing

Testing outputs of LLMs is important. It reduces risk and it ensures that you understand the content output. Testing can take many forms. You might reason through an LLMs outputs critically assessing each of its assertions either using logic or cross checking against other material. Testing of LLM generated code might be similar to how you'd test your own code, your could write unit tests or try out the produced application, testing different usage scenarios and checking for errors.  

### Understanding consequences and setting a risk tolerance

In some cases you maybe able to tolerate some error. For example, if you are learning about trigonmetry and you are going to read a wide variety of material it maybe ok if an LLM misleads you as you will discover this error in further learning or exercises. In other cases errors could be catastrophic. For example and LLM may incorrectly analyze a series of medical documents and recommend a dangerous or incorrect treatement plan. Understanding consequences and set an appropriate risk tolerance when using LLMs.

### Censoring Data

Generating 



### Working on Adjacent problems




Exercises:

1. Find the privacy terms and conditions for OpenAIs ChatGPT
2. Try to find a mistake in an LLM by asking a really tricky question at the edge of your domain knowledge?
3. What would be an example of some data from your context you should not send to an LLM?
4. Can you think of any safe ways of engaging an LLM around your problem when there maybe a sensitive aspect? 
5. How could you test LLM output in your context?